=== CentOS8+ELK环境配置

==== 安装ELK

同 <<_centos7elk环境配置>>

==== Logstash

[red, big]#“如何将Nginx的Web日志导入Elasticsearch？”#

===== 日志处理管道

日志记录增加到Elasticsearch时，需要使用管道实时清洗、处理日志格式。

.Nginx日志格式
[source, text]
----
log_format  main  '$host $remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';
----

执行以下命令，在Elasticsearch中增加管道：

[source, bash]
----
curl -H "Content-Type: application/json" -X PUT http://0.0.0.0:9200/_ingest/pipeline/web_log -d '
{
    "description": "",
    "processors": [
        {
            "dissect": {
                "field": "message",
                "pattern": "%{domain} %{server_port} %{client_ip} - - [%{web_timestamp}] \"%{method} %{uri} HTTP/%{version}\" %{status} %{response_time} %{body_bytes} \"%{referer}\" \"%{user_agent}\" \"%{x_forwarded_for}\" %{server_ip}"
            }
        },
        {
            "grok": {
                "field": "uri",
                "on_failure": [
                    {
                "set": {
                    "field": "ext",
                    "value": ""
                }
                    },
                    {
                "set": {
                    "field": "url_base",
                    "value": "{{uri}}"
                }
                    }
                ],
                "patterns": [
                    "%{GREEDYDATA:url_base}\\.%{WORD:ext}"
                ]
            }
        },
        {
            "geoip": {
                "field": "client_ip",
                "target_field": "client_ip_geo"
            }
        },
        {
            "geoip": {
                "field": "server_ip",
                "target_field": "server_ip_geo"
            }
        },
        {
            "user_agent": {
                "field": "user_agent"
            }
        },
        {
            "date": {
                "field": "web_timestamp",
                "target_field": "@timestamp",
                "formats": [
                    "dd/MMM/yyyy:HH:mm:ss Z"
                ],
                "timezone" : "Asia/Shanghai"
            }
        },
        {
            "convert": {
                "field": "server_port",
                "type": "integer"
            }
        },
        {
            "convert": {
                "field": "status",
                "type": "integer"
            }
        },
        {
            "convert": {
                "field": "response_time",
                "type": "float"
            }
        },
        {
            "convert": {
                "field": "body_bytes",
                "type": "integer"
            }
        },
        {
            "set": {
                "field": "hour_of_day",
                "value": 12
            }
        },
        {
            "grok": {
                "field": "web_timestamp",
                "patterns": [
                    "\\d+/\\w+/\\d+:%{NUMBER:hour_of_day:int}:\\d+:\\d+ \\+\\d+"
                ]
            }
        }
    ]
}
'
----

===== 准备日志文件

日志文件存放路径:: ~/es_log/nginx_logs

[source, console]
----
[root@dell7 nginx_logs]# ls
access.log-20200501  access.log-20200512  access.log-20200523  access.log-20200603
access.log-20200502  access.log-20200513  access.log-20200524  access.log-20200604
access.log-20200503  access.log-20200514  access.log-20200525  access.log-20200605
access.log-20200504  access.log-20200515  access.log-20200526  access.log-20200606
access.log-20200505  access.log-20200516  access.log-20200527  access.log-20200607
access.log-20200506  access.log-20200517  access.log-20200528  access.log-20200608
access.log-20200507  access.log-20200518  access.log-20200529  access.log-20200609
access.log-20200508  access.log-20200519  access.log-20200530  access.log-20200610
access.log-20200509  access.log-20200520  access.log-20200531
access.log-20200510  access.log-20200521  access.log-20200601
access.log-20200511  access.log-20200522  access.log-20200602
----

===== 配置文件

[source, bash]
----
mkdir -p ~/es_log

cat << EOF > ~/es_log/logstash.conf
input{
  file {
    # https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html#plugins-inputs-file-delimiter
    # 默认以"\n"分割，文件必须至少一个换行符
    path => "${HOME}/es_log/nginx_logs/access.log*"
    start_position => "end"
    stat_interval => "1 second"
  }
}
output{
    elasticsearch {
            #这里可以是数组，可以是多个节点的地址，会自动启用负载均衡
            hosts => ["0.0.0.0:9200"]
            index => "foo_com"
            pipeline => "web_log"
    }
    #file  {
    #    path => "/var/log/logstash.log"
    #    codec => json
    #}
}
EOF
----

[IMPORTANT]
====
如果Elasticsearch的 `"network.host"` 参数值为具体的IP地址，比如 `"172.24.109.12"`。

那么，Logstash配置中的 `output.elasticsearch.hosts` 需要设置为 `["172.24.109.12:9200"]`。
====

===== 导入Nginx日志到Elasticsearch

导入数据到ES：

[source, bash]
/usr/share/logstash/bin/logstash -f ~/es_log/logstash.conf


每次运行的结果会记录到文件：

/usr/share/logstash/data/plugins/inputs/file/.sincedb_100caf6f694120ba2483577719e4a564

文件内容为：

[source, text]
----
270004749 0 2049 584450300 1592393767.626652 /root/es_log/nginx_logs/access.log-20200501
272281186 0 2049 471367497 1592394059.861428 /root/es_log/nginx_logs/access.log-20200502
----

如果想重新导入，删除 .sincedb 文件即可。

